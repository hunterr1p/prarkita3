import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Model
import warnings
warnings.filterwarnings('ignore')

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
ratings = pd.read_csv('rating.csv')
movies = pd.read_csv('movie.csv')

print("=" * 60)
print("–î–ê–¢–ê–°–ï–¢ MOVIELENS 25M - –ê–ù–ê–õ–ò–ó")
print("=" * 60)

print("\nüìä Ratings –¥–∞–Ω–Ω—ã–µ:")
print(ratings.head())
print(f"\n–§–æ—Ä–º–∞ ratings: {ratings.shape}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {ratings['userId'].nunique():,}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤: {ratings['movieId'].nunique():,}")
print(f"–í—Å–µ–≥–æ –æ—Ü–µ–Ω–æ–∫: {len(ratings):,}")

print("\nüé¨ Movies –¥–∞–Ω–Ω—ã–µ:")
print(movies.head())
print(f"\n–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∂–∞–Ω—Ä–æ–≤: {movies['genres'].str.split('|').explode().nunique()}")

# –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç–∏
num_users = ratings['userId'].nunique()
num_movies = ratings['movieId'].nunique()
total_possible = num_users * num_movies
sparsity = (1 - len(ratings) / total_possible) * 100

print(f"\nüìà –†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å –º–∞—Ç—Ä–∏—Ü—ã:")
print(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ √ó –§–∏–ª—å–º—ã = {num_users:,} √ó {num_movies:,} = {total_possible:,} –≤–æ–∑–º–æ–∂–Ω—ã—Ö –æ—Ü–µ–Ω–æ–∫")
print(f"–§–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ü–µ–Ω–æ–∫: {len(ratings):,}")
print(f"–†–∞–∑—Ä–µ–∂–µ–Ω–Ω–æ—Å—Ç—å: {sparsity:.4f}%")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Ü–µ–Ω–æ–∫
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
ratings['rating'].hist(bins=50, edgecolor='black', alpha=0.7)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Ü–µ–Ω–æ–∫', fontsize=12)
plt.xlabel('–û—Ü–µ–Ω–∫–∞')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')

plt.subplot(1, 3, 2)
ratings_per_user = ratings.groupby('userId')['rating'].count()
ratings_per_user.hist(bins=50, edgecolor='black', alpha=0.7, log=True)
plt.title('–û—Ü–µ–Ω–æ–∫ –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (log scale)', fontsize=12)
plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞ (log)')

plt.subplot(1, 3, 3)
ratings_per_movie = ratings.groupby('movieId')['rating'].count()
ratings_per_movie.hist(bins=50, edgecolor='black', alpha=0.7, log=True)
plt.title('–û—Ü–µ–Ω–æ–∫ –Ω–∞ —Ñ–∏–ª—å–º (log scale)', fontsize=12)
plt.xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞ (log)')

plt.tight_layout()
plt.show()
print("=" * 60)
print("–ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ù–ï–ô–†–û–°–ï–¢–ò")
print("=" * 60)

# –î–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –≤–æ–∑—å–º–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ (–º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞)
SAMPLE_SIZE = 200000  # –£–≤–µ–ª–∏—á—å—Ç–µ –¥–æ 1-2 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –¥–ª—è production-–∫–∞—á–µ—Å—Ç–≤–∞
print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É –∏–∑ {SAMPLE_SIZE:,} –æ—Ü–µ–Ω–æ–∫")

ratings_sample = ratings.sample(n=SAMPLE_SIZE, random_state=42)

# –ó–∞–∫–æ–¥–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ñ–∏–ª—å–º—ã –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
user_encoder = LabelEncoder()
movie_encoder = LabelEncoder()

ratings_sample['user_idx'] = user_encoder.fit_transform(ratings_sample['userId'])
ratings_sample['movie_idx'] = movie_encoder.fit_transform(ratings_sample['movieId'])

num_users_encoded = len(user_encoder.classes_)
num_movies_encoded = len(movie_encoder.classes_)

print(f"\n–ü–æ—Å–ª–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è:")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {num_users_encoded}")
print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤: {num_movies_encoded}")

# –†–∞–∑–¥–µ–ª–∏–º –Ω–∞ train/test
train, test = train_test_split(
    ratings_sample[['user_idx', 'movie_idx', 'rating']],
    test_size=0.2,
    random_state=42
)

print(f"\n–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(train):,} –∑–∞–ø–∏—Å–µ–π")
print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(test):,} –∑–∞–ø–∏—Å–µ–π")

# –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –æ—Ü–µ–Ω–∫–∏ (–æ—Ç 0.5-5.0 –∫ 0-1)
rating_min, rating_max = 0.5, 5.0
train['rating_norm'] = (train['rating'] - rating_min) / (rating_max - rating_min)
test['rating_norm'] = (test['rating'] - rating_min) / (rating_max - rating_min)

print(f"\n–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ü–µ–Ω–æ–∫:")
print(f"–ò—Å—Ö–æ–¥–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: {rating_min}-{rating_max}")
print(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω: {train['rating_norm'].min():.2f}-{train['rating_norm'].max():.2f}")
print("=" * 60)
print("–ü–û–°–¢–†–û–ï–ù–ò–ï –ù–ï–ô–†–û–°–ï–¢–ï–í–û–ô –ú–û–î–ï–õ–ò")
print("=" * 60)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
EMBEDDING_SIZE = 50
DROPOUT_RATE = 0.3
LEARNING_RATE = 0.001

def build_neural_cf_model(num_users, num_movies, embedding_size=EMBEDDING_SIZE):
    """
    –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –º–æ–¥–µ–ª—å –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    Architecture: Embedding -> Flatten -> Concatenate -> Dense -> Output
    """
    # –í—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–∏
    user_input = layers.Input(shape=(1,), name='user_input')
    movie_input = layers.Input(shape=(1,), name='movie_input')

    # –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–∏
    user_embedding = layers.Embedding(
        input_dim=num_users,
        output_dim=embedding_size,
        name='user_embedding'
    )(user_input)

    movie_embedding = layers.Embedding(
        input_dim=num_movies,
        output_dim=embedding_size,
        name='movie_embedding'
    )(movie_input)

    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –≤–µ–∫—Ç–æ—Ä—ã
    user_vector = layers.Flatten()(user_embedding)
    movie_vector = layers.Flatten()(movie_embedding)

    # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º –≤–µ–∫—Ç–æ—Ä—ã
    concatenated = layers.Concatenate()([user_vector, movie_vector])

    # –ü–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–µ —Å–ª–æ–∏
    dense = layers.Dense(128, activation='relu')(concatenated)
    dropout = layers.Dropout(DROPOUT_RATE)(dense)
    dense2 = layers.Dense(64, activation='relu')(dropout)
    dropout2 = layers.Dropout(DROPOUT_RATE)(dense2)
    dense3 = layers.Dense(32, activation='relu')(dropout2)

    # –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏ 0-1)
    output = layers.Dense(1, activation='sigmoid', name='rating_output')(dense3)

    # –°–æ–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å
    model = Model(inputs=[user_input, movie_input], outputs=output)

    return model

# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
model = build_neural_cf_model(num_users_encoded, num_movies_encoded)
model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),
    loss='mse',  # Mean Squared Error –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏
    metrics=['mae', 'mse']  # Mean Absolute Error –∏ MSE
)

print("–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏:")
model.summary()

# –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É
keras.utils.plot_model(
    model,
    to_file='model_architecture.png',
    show_shapes=True,
    show_layer_names=True
)
print("\n‚úÖ –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –º–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'model_architecture.png'")
print("=" * 60)
print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò")
print("=" * 60)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
train_user_data = train['user_idx'].values
train_movie_data = train['movie_idx'].values
train_ratings = train['rating_norm'].values

test_user_data = test['user_idx'].values
test_movie_data = test['movie_idx'].values
test_ratings = test['rating_norm'].values

# Callback'–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
early_stopping = keras.callbacks.EarlyStopping(
    monitor='val_loss',
    patience=5,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=3,
    min_lr=0.00001,
    verbose=1
)

# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
history = model.fit(
    x=[train_user_data, train_movie_data],
    y=train_ratings,
    batch_size=256,
    epochs=30,
    validation_split=0.1,
    callbacks=[early_stopping, reduce_lr],
    verbose=1
)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='–û–±—É—á–∞—é—â–∞—è')
plt.plot(history.history['val_loss'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è')
plt.title('–§—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å (MSE)')
plt.xlabel('–≠–ø–æ—Ö–∞')
plt.ylabel('–ü–æ—Ç–µ—Ä–∏')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history.history['mae'], label='–û–±—É—á–∞—é—â–∞—è')
plt.plot(history.history['val_mae'], label='–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è')
plt.title('–°—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ (MAE)')
plt.xlabel('–≠–ø–æ—Ö–∞')
plt.ylabel('MAE')
plt.legend()
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
print("=" * 60)
print("–û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò")
print("=" * 60)

# –û—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
test_loss, test_mae, test_mse = model.evaluate(
    [test_user_data, test_movie_data],
    test_ratings,
    verbose=0
)

# –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –∏—Å—Ö–æ–¥–Ω—ã–π –º–∞—Å—à—Ç–∞–± –æ—Ü–µ–Ω–æ–∫
test_mae_original = test_mae * (rating_max - rating_min)
test_rmse_original = np.sqrt(test_mse) * (rating_max - rating_min)

print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ:")
print(f"MAE (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π): {test_mae:.4f}")
print(f"MAE (–≤ –∏—Å—Ö–æ–¥–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ 0.5-5.0): {test_mae_original:.4f}")
print(f"RMSE (–≤ –∏—Å—Ö–æ–¥–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ): {test_rmse_original:.4f}")

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
def get_recommendations(user_id, top_n=10, exclude_rated=True):
    """
    –ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω–¥–µ–∫—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_idx = user_encoder.transform([user_id])[0]

    # –í—Å–µ —Ñ–∏–ª—å–º—ã
    all_movie_indices = np.arange(num_movies_encoded)
    user_indices = np.full(num_movies_encoded, user_idx)

    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ–∏–ª—å–º–æ–≤
    predictions = model.predict(
        [user_indices, all_movie_indices],
        batch_size=1024,
        verbose=0
    ).flatten()

    # –ü–æ–ª—É—á–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–µ movieId
    original_movie_ids = movie_encoder.inverse_transform(all_movie_indices)

    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    recommendations = pd.DataFrame({
        'movieId': original_movie_ids,
        'predicted_rating': predictions * (rating_max - rating_min) + rating_min
    })

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ñ–∏–ª—å–º–∞—Ö
    recommendations = recommendations.merge(movies, on='movieId')

    # –ò—Å–∫–ª—é—á–∞–µ–º —É–∂–µ –æ—Ü–µ–Ω–µ–Ω–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º —Ñ–∏–ª—å–º—ã
    if exclude_rated:
        rated_movies = ratings[ratings['userId'] == user_id]['movieId'].tolist()
        recommendations = recommendations[~recommendations['movieId'].isin(rated_movies)]

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–µ
    recommendations = recommendations.sort_values('predicted_rating', ascending=False)

    return recommendations.head(top_n)

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ—Ö–æ–∂–∏—Ö —Ñ–∏–ª—å–º–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
def get_similar_movies(movie_id, top_n=10):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ —Ñ–∏–ª—å–º—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π (—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
    """
    movie_idx = movie_encoder.transform([movie_id])[0]

    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–ª–æ–∏
    movie_embedding_layer = model.get_layer('movie_embedding')
    movie_embeddings = movie_embedding_layer.get_weights()[0]

    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    target_embedding = movie_embeddings[movie_idx].reshape(1, -1)
    similarities = np.dot(movie_embeddings, target_embedding.T).flatten()

    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-N –ø–æ—Ö–æ–∂–∏—Ö —Ñ–∏–ª—å–º–æ–≤ (–∏—Å–∫–ª—é—á–∞—è —Å–∞–º —Ñ–∏–ª—å–º)
    similar_indices = similarities.argsort()[-(top_n+1):-1][::-1]
    similar_movie_ids = movie_encoder.inverse_transform(similar_indices)

    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    similar_movies = pd.DataFrame({
        'movieId': similar_movie_ids,
        'similarity': similarities[similar_indices]
    }).merge(movies, on='movieId')

    return similar_movies

# –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
print("\nüéØ –ü—Ä–∏–º–µ—Ä —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è #1:")
user_id_example = 1  # –ú–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å –Ω–∞ –ª—é–±–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

try:
    recommendations = get_recommendations(user_id_example, top_n=5)
    print(f"\n–¢–æ–ø-5 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id_example}:")
    for i, row in recommendations.iterrows():
        print(f"{i+1}. {row['title']} | –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {row['predicted_rating']:.2f} | –ñ–∞–Ω—Ä—ã: {row['genres']}")
except Exception as e:
    print(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id_example} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –≤—ã–±–æ—Ä–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–≥–æ.")

# –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Ñ–∏–ª—å–º–æ–≤
print("\nüîç –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —Ñ–∏–ª—å–º–æ–≤:")
# –ù–∞–π–¥–µ–º ID —Ñ–∏–ª—å–º–∞ "Toy Story" (1995)
toy_story = movies[movies['title'].str.contains('Toy Story') & movies['title'].str.contains('1995')]
if not toy_story.empty:
    toy_story_id = toy_story.iloc[0]['movieId']
    print(f"–ù–∞—Ö–æ–¥–∏–º —Ñ–∏–ª—å–º—ã, –ø–æ—Ö–æ–∂–∏–µ –Ω–∞: {toy_story.iloc[0]['title']}")

    similar = get_similar_movies(toy_story_id, top_n=5)
    for i, row in similar.iterrows():
        print(f"{i+1}. {row['title']} | –°—Ö–æ–¥—Å—Ç–≤–æ: {row['similarity']:.3f}")
else:
    print("–§–∏–ª—å–º 'Toy Story' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∞–∑–µ")

# –ú–∞—Å—Å–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
print("\nüìã –ú–∞—Å—Å–æ–≤–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π:")
sample_users = ratings['userId'].drop_duplicates().sample(3, random_state=42).tolist()

for user in sample_users:
    try:
        recs = get_recommendations(user, top_n=3)
        print(f"\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user}:")
        for _, row in recs.iterrows():
            print(f"  ‚Ä¢ {row['title'][:50]}... ({row['predicted_rating']:.2f})")
    except:
        print(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user} –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ")
print("=" * 60)
print("–£–õ–£–ß–®–ï–ù–ù–ê–Ø –ú–û–î–ï–õ–¨: NEURAL COLLABORATIVE FILTERING")
print("=" * 60)

def build_ncf_model(num_users, num_movies, embedding_size=50):
    """
    –£–ª—É—á—à–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å Neural Collaborative Filtering
    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—á–Ω—É—é —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—é –∏ –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω—ã–π –ø–µ—Ä—Ü–µ–ø—Ç—Ä–æ–Ω
    """
    # –í—Ö–æ–¥–Ω—ã–µ —Å–ª–æ–∏
    user_input = layers.Input(shape=(1,), name='user_input')
    movie_input = layers.Input(shape=(1,), name='movie_input')

    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –º–∞—Ç—Ä–∏—á–Ω–æ–π —Ñ–∞–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (GMF)
    user_embedding_mf = layers.Embedding(num_users, embedding_size, name='user_embedding_mf')(user_input)
    movie_embedding_mf = layers.Embedding(num_movies, embedding_size, name='movie_embedding_mf')(movie_input)

    # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è MLP
    user_embedding_mlp = layers.Embedding(num_users, embedding_size*2, name='user_embedding_mlp')(user_input)
    movie_embedding_mlp = layers.Embedding(num_movies, embedding_size*2, name='movie_embedding_mlp')(movie_input)

    # GMF –ø—É—Ç—å: element-wise –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
    user_vec_mf = layers.Flatten()(user_embedding_mf)
    movie_vec_mf = layers.Flatten()(movie_embedding_mf)
    mf_vector = layers.Multiply()([user_vec_mf, movie_vec_mf])

    # MLP –ø—É—Ç—å
    user_vec_mlp = layers.Flatten()(user_embedding_mlp)
    movie_vec_mlp = layers.Flatten()(movie_embedding_mlp)
    mlp_vector = layers.Concatenate()([user_vec_mlp, movie_vec_mlp])

    # MLP —Å–ª–æ–∏
    mlp_vector = layers.Dense(128, activation='relu')(mlp_vector)
    mlp_vector = layers.Dropout(0.3)(mlp_vector)
    mlp_vector = layers.Dense(64, activation='relu')(mlp_vector)
    mlp_vector = layers.Dropout(0.2)(mlp_vector)
    mlp_vector = layers.Dense(32, activation='relu')(mlp_vector)

    # –ö–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä—É–µ–º GMF –∏ MLP
    concatenated = layers.Concatenate()([mf_vector, mlp_vector])

    # –§–∏–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–∏
    output = layers.Dense(16, activation='relu')(concatenated)
    output = layers.Dropout(0.1)(output)
    output = layers.Dense(1, activation='sigmoid', name='output')(output)

    model = Model(inputs=[user_input, movie_input], outputs=output)

    return model

# –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
print("\n–°–æ–∑–¥–∞–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é NCF –º–æ–¥–µ–ª—å...")
ncf_model = build_ncf_model(num_users_encoded, num_movies_encoded, embedding_size=32)
ncf_model.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='mse',
    metrics=['mae']
)

ncf_model.summary()

# –ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–µ–±–æ–ª—å—à–æ–º —á–∏—Å–ª–µ —ç–ø–æ—Ö
print("\n–ë—ã—Å—Ç—Ä–æ–µ –æ–±—É—á–µ–Ω–∏–µ NCF –º–æ–¥–µ–ª–∏ (5 —ç–ø–æ—Ö)...")
ncf_history = ncf_model.fit(
    [train_user_data, train_movie_data],
    train_ratings,
    batch_size=512,
    epochs=5,
    validation_split=0.1,
    verbose=1
)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
print("\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
print("-" * 40)
print("–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å vs NCF –º–æ–¥–µ–ª—å")
print("(—á–µ–º –Ω–∏–∂–µ –º–µ—Ç—Ä–∏–∫–∏, —Ç–µ–º –ª—É—á—à–µ)")

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏
base_pred = model.predict([test_user_data[:1000], test_movie_data[:1000]]).flatten()
base_pred = base_pred * (rating_max - rating_min) + rating_min
base_mae = np.mean(np.abs(base_pred - test['rating'].values[:1000]))

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è NCF –º–æ–¥–µ–ª–∏
ncf_pred = ncf_model.predict([test_user_data[:1000], test_movie_data[:1000]]).flatten()
ncf_pred = ncf_pred * (rating_max - rating_min) + rating_min
ncf_mae = np.mean(np.abs(ncf_pred - test['rating'].values[:1000]))

print(f"\nMAE –Ω–∞ 1000 —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø—Ä–∏–º–µ—Ä–∞—Ö:")
print(f"  –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: {base_mae:.4f}")
print(f"  NCF –º–æ–¥–µ–ª—å: {ncf_mae:.4f}")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
plt.figure(figsize=(10, 4))

plt.subplot(1, 2, 1)
plt.scatter(test['rating'].values[:100], base_pred[:100], alpha=0.5)
plt.plot([0.5, 5], [0.5, 5], 'r--', alpha=0.5)
plt.title('–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: –§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑')
plt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞')
plt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.scatter(test['rating'].values[:100], ncf_pred[:100], alpha=0.5)
plt.plot([0.5, 5], [0.5, 5], 'r--', alpha=0.5)
plt.title('NCF –º–æ–¥–µ–ª—å: –§–∞–∫—Ç vs –ü—Ä–æ–≥–Ω–æ–∑')
plt.xlabel('–§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞')
plt.ylabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
print("=" * 60)
print("–°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ò –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
print("=" * 60)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
model.save('neural_cf_recommender.h5')
print("‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ 'neural_cf_recommender.h5'")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏
import pickle
with open('user_encoder.pkl', 'wb') as f:
    pickle.dump(user_encoder, f)
with open('movie_encoder.pkl', 'wb') as f:
    pickle.dump(movie_encoder, f)
print("‚úÖ –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
sample_recommendations = []
for user_id in ratings['userId'].unique()[:10]:
    try:
        recs = get_recommendations(user_id, top_n=3)
        for _, row in recs.iterrows():
            sample_recommendations.append({
                'user_id': user_id,
                'movie_id': row['movieId'],
                'title': row['title'],
                'predicted_rating': row['predicted_rating'],
                'genres': row['genres']
            })
    except:
        continue

recs_df = pd.DataFrame(sample_recommendations)
recs_df.to_csv('sample_recommendations.csv', index=False)
print(f"‚úÖ –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'sample_recommendations.csv' ({len(recs_df)} –∑–∞–ø–∏—Å–µ–π)")
print(f"–ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ (MAE): {test_mae_original:.3f} (–≤ —à–∫–∞–ª–µ 0.5-5.0)")
